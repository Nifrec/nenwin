{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "f50bd5474255f82aa829301912ce59e29110123be660cf8d7583f66a20371684"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Nenwin-project (NEural Networks WIthout Neurons) for\n",
    "the AI Honors Academy track 2020-2021 at the TU Eindhoven.\n",
    "\n",
    "Author: Lulof Pirée\n",
    "March 2021\n",
    "\n",
    "Copyright (C) 2020 Lulof Pirée, Teun Schilperoort\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU Affero General Public License as published\n",
    "by the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU Affero General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Affero General Public License\n",
    "along with this program.  If not, see <https://www.gnu.org/licenses/>."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Backprob experiment\n",
    "\n",
    "This file provides a full run of backpropagating though an entire `NenwinModel`.\n",
    "\n",
    "Author: Lulof Pirée"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchviz\n",
    "\n",
    "from nenwin.all_particles import Marble, Node, MarbleEaterNode, MarbleEmitterNode\n",
    "from nenwin.model import NenwinModel\n",
    "from nenwin.attraction_functions.attraction_functions import NewtonianGravity"
   ]
  },
  {
   "source": [
    "## Approach\n",
    "\n",
    "1. Decide how to capture output and network architecture.\n",
    "1. Set up the model and the particles.\n",
    "1. Add a loss function.\n",
    "1. Visualize the model.\n",
    "1. Run the model.\n",
    "1. Backpropagate and update.\n",
    "1. Compare differences in model.\n",
    "\n",
    "### Design\n",
    "Let's create a ring of Nodes, say 5 of them, and put one Marble in there. Not in the center, just somewhere within the circle of space (let's keep it 2D) enclosed by the ring of Nodes.\n",
    "Now let the loss simply be the distance of the Marble to the center of the circle **after 5 seconds from the start**.\n",
    "\n",
    "One likely result -- if everything works -- is that the Marble simply is placed stationary on this position. But that would already be some successfull optimization!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "zero = torch.tensor([0, 0], dtype=torch.float)\n",
    "mass = 10\n",
    "\n",
    "center = torch.tensor([0, 0], dtype=torch.float, requires_grad=False)\n",
    "node_positions = [torch.tensor(pos, dtype=torch.float) for pos in ((0, 10), (10, 0), (0, -10), (-10, 0))]\n",
    "marble_pos = torch.tensor([2, 2], dtype=torch.float)\n",
    "\n",
    "nodes = [Node(pos, zero, zero, mass, NewtonianGravity(), 1, 1, 1, 1) \n",
    "    for pos in node_positions]\n",
    "\n",
    "marble = Marble(marble_pos, zero, zero, mass, NewtonianGravity(), None)\n",
    "\n",
    "model = NenwinModel(nodes, (marble,))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "step_size = 5\n",
    "\n",
    "for epoch in range(25):\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    marble.zero_grad(set_to_none=True)\n",
    "    print(f\"Before reset epoch {epoch}\")\n",
    "    print(marble.init_pos._version)\n",
    "\n",
    "\n",
    "    # model.reset()\n",
    "    marble.zero_grad(set_to_none=True)\n",
    "    marble.reset()\n",
    "    \n",
    "    for node in nodes:\n",
    "        node.reset()\n",
    "        node.zero_grad(set_to_none=True)\n",
    "    \n",
    "    print(f\"being epoch {epoch}\")\n",
    "    print(marble.init_pos._version)\n",
    "    \n",
    "    t = 0\n",
    "    while t <= 5:\n",
    "        model.make_timestep(step_size)\n",
    "        t += step_size\n",
    "\n",
    "    # loss = torch.abs(marble.pos[0]) #+ torch.abs(marble.pos[1])\n",
    "    # loss = torch.mean(torch.pow(marble.pos - torch.tensor([0, 0], dtype=torch.float), 2))\n",
    "    # print(loss)\n",
    "\n",
    "    print(f\"end epoch {epoch}\")\n",
    "    print(marble.init_pos._version)\n",
    "    marble.pos[0].backward()\n",
    "    # loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 90,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before reset epoch 0\n",
      "0\n",
      "being epoch 0\n",
      "0\n",
      "end epoch 0\n",
      "0\n",
      "Before reset epoch 1\n",
      "1\n",
      "being epoch 1\n",
      "1\n",
      "end epoch 1\n",
      "1\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []] is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-bcad25491017>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"end epoch {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mmarble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;31m# loss.backward(retain_graph=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []] is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ]
  },
  {
   "source": [
    "## Try it with a single Marble -- do the same errors happen?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "end epoch 0\n0\n0\n0\nend epoch 1\n1\n0\n1\nError occured\nprev_prev: 0\nprev: 0\ninit: 1\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fcaa6b81750>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.46.0 (0)\n -->\n<!-- Pages: 1 -->\n<svg width=\"223pt\" height=\"200pt\"\n viewBox=\"0.00 0.00 223.00 200.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 196)\">\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-196 219,-196 219,4 -4,4\"/>\n<!-- 140508251268816 -->\n<g id=\"node1\" class=\"node\">\n<title>140508251268816</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"164,-21 51,-21 51,0 164,0 164,-21\"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\">CloneBackward</text>\n</g>\n<!-- 140508251268496 -->\n<g id=\"node2\" class=\"node\">\n<title>140508251268496</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"164,-78 51,-78 51,-57 164,-57 164,-78\"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\">CloneBackward</text>\n</g>\n<!-- 140508251268496&#45;&gt;140508251268816 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140508251268496&#45;&gt;140508251268816</title>\n<path fill=\"none\" stroke=\"black\" d=\"M107.5,-56.92C107.5,-49.91 107.5,-40.14 107.5,-31.47\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"111,-31.34 107.5,-21.34 104,-31.34 111,-31.34\"/>\n</g>\n<!-- 140508304795728 -->\n<g id=\"node3\" class=\"node\">\n<title>140508304795728</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"164,-135 51,-135 51,-114 164,-114 164,-135\"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\">CloneBackward</text>\n</g>\n<!-- 140508304795728&#45;&gt;140508251268496 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140508304795728&#45;&gt;140508251268496</title>\n<path fill=\"none\" stroke=\"black\" d=\"M107.5,-113.92C107.5,-106.91 107.5,-97.14 107.5,-88.47\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"111,-88.34 107.5,-78.34 104,-88.34 111,-88.34\"/>\n</g>\n<!-- 140507994972432 -->\n<g id=\"node4\" class=\"node\">\n<title>140507994972432</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"215,-192 0,-192 0,-171 215,-171 215,-192\"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-178.4\" font-family=\"Times,serif\" font-size=\"12.00\">_InitialValueParticle__init_acc (2)</text>\n</g>\n<!-- 140507994972432&#45;&gt;140508304795728 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140507994972432&#45;&gt;140508304795728</title>\n<path fill=\"none\" stroke=\"black\" d=\"M107.5,-170.92C107.5,-163.91 107.5,-154.14 107.5,-145.47\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"111,-145.34 107.5,-135.34 104,-145.34 111,-145.34\"/>\n</g>\n</g>\n</svg>\n"
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "marble = Marble(marble_pos, zero, zero, mass, NewtonianGravity(), None)\n",
    "optimizer = torch.optim.Adam(marble.parameters())\n",
    "step_time=0.5\n",
    "\n",
    "\n",
    "for epoch in range(25):\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    marble.zero_grad(set_to_none=True)\n",
    "    marble.reset()\n",
    "    # print(vars(marble))\n",
    "    t = 0\n",
    "    while t <= 5:\n",
    "        marble.update_movement(time_passed=step_time)\n",
    "        t += step_size\n",
    "\n",
    "    loss = torch.mean(torch.pow(marble.pos - torch.tensor([0, 0], dtype=torch.float), 2))\n",
    "\n",
    "    print(f\"end epoch {epoch}\")\n",
    "    print(marble.init_pos._version)\n",
    "    print(marble._PhysicalParticle__mass._version)\n",
    "    print(marble._InitialValueParticle__init_pos._version)\n",
    "\n",
    "    try:\n",
    "        loss.backward()\n",
    "        del loss\n",
    "        optimizer.step()\n",
    "    except:\n",
    "        print(\"Error occured\")\n",
    "        break\n",
    "print(\"prev_prev:\", marble._prev_prev_acc._version)\n",
    "print(\"prev:\", marble._prev_acc._version)\n",
    "print(\"init:\", marble.init_acc._version)\n",
    "torchviz.make_dot(marble._prev_acc, params=dict(marble.named_parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on function make_dot in module torchviz.dot:\n\nmake_dot(var, params=None)\n    Produces Graphviz representation of PyTorch autograd graph.\n    \n    Blue nodes are the Variables that require grad, orange are Tensors\n    saved for backward in torch.autograd.Function\n    \n    Args:\n        var: output Variable\n        params: dict of (name, Variable) to add names to node that\n            require grad (TODO: make optional)\n\n"
     ]
    }
   ],
   "source": [
    "help(torchviz.make_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "a = torch.tensor([1.0], requires_grad = True)\n",
    "a[0] += 1\n",
    "a._version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "b = a.clone()\n",
    "b._version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.my_param = nn.Parameter(torch.tensor([1.0], requires_grad=True))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self.my_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2\n0\n0\n"
     ]
    }
   ],
   "source": [
    "mm = MyModule()\n",
    "optim = torch.optim.Adam(mm.parameters())\n",
    "\n",
    "for epoch in range(2):\n",
    "    optim.zero_grad()\n",
    "    loss = mm(torch.tensor([2.0]))\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "print(mm.my_param._version)\n",
    "print(mm(torch.tensor([2.0]))._version)\n",
    "print(loss._version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}