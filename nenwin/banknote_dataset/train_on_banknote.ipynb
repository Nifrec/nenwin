{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0f50bd5474255f82aa829301912ce59e29110123be660cf8d7583f66a20371684",
   "display_name": "Python 3.7.9 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Nenwin-project (NEural Networks WIthout Neurons) for\n",
    "the AI Honors Academy track 2020-2021 at the TU Eindhoven.\n",
    "\n",
    "Authors: Lulof Pirée\n",
    "\n",
    "May 2021\n",
    "\n",
    "Copyright (C) 2021 Lulof Pirée\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU Affero General Public License as published\n",
    "by the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU Affero General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Affero General Public License\n",
    "along with this program.  If not, see <https://www.gnu.org/licenses/>."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Banknote dataset with Nenwin\n",
    "\n",
    "Dataset source: https://code.datasciencedojo.com/datasciencedojo/datasets/blob/master/Banknote%20Authentication/data_banknote_authentication.txt\n",
    "See also:\n",
    "* https://jamesmccaffrey.wordpress.com/2020/08/18/in-the-banknote-authentication-dataset-class-0-is-genuine-authentic/\n",
    "* https://www.researchgate.net/publication/266673146_Banknote_Authentication\n",
    "\n",
    "This dataset has:\n",
    "* 1372 samples, of two classes:\n",
    "    * class 0 (Genuine): 762 samples \n",
    "    * class 1 (Forgery): 610 samples\n",
    "\n",
    "Features:\n",
    "* 0: variance (float)\n",
    "* 1: skewness (float)\n",
    "* 2: curtosis (float)\n",
    "* 3: entropy  (float)\n",
    "\n",
    "\n",
    "**Goal**: classify which banknotes are real and which ones are forgeries,\n",
    "\n",
    "based on the four features (that are derived from an image, using the 'Wavelet Transform\")."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Iterable\n",
    "from numbers import Number\n",
    "\n",
    "from nenwin.constants import BANKNOTE_CHECKPOINT_DIR\n",
    "from nenwin.all_particles import Marble, Node, MarbleEmitterNode, MarbleEaterNode\n",
    "from nenwin.model import NenwinModel\n",
    "from nenwin.input_placer import InputPlacer\n",
    "from nenwin.grid_input_placer import VelInputPlacer\n",
    "from nenwin.attraction_functions.attraction_functions import NewtonianGravity, AttractionFunction\n",
    "from nenwin.backprop.filename_gen import FilenameGenerator\n",
    "from nenwin.backprop.trainer import NenwinTrainer\n",
    "from nenwin.backprop.training_stats import TrainingStats\n",
    "from nenwin.backprop.loss_function import NenwinLossFunction\n",
    "from nenwin.banknote_dataset.load_dataset import load_banknote_dataset, BanknoteDataset\n",
    "from nenwin.creation_functions import gen_nodes, gen_eater_nodes"
   ]
  },
  {
   "source": [
    "## Architecture generation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_architecture() -> Tuple[NenwinModel, VelInputPlacer, Tuple[Node]]:\n",
    "    \"\"\"\n",
    "    Generate the following architecture:\n",
    "    * The input region is at (-2.5, -1) and has size (5, 2) \n",
    "        (So it has vertices {(-2.5, -1), (-2.5, 1), (2.5, -1), (2.5, 1)})\n",
    "    * There are two MarbleEaterNodes, at (-10, 0) and (10, 0)\n",
    "    * There are four normal Nodes, at (0, -5), (-5, 0), (5, 0) and (0, 5).\n",
    "\n",
    "    Returns:\n",
    "    * Model holding the architecture descibed above\n",
    "    * VelInputPlacer with the input region as described above.\n",
    "    * Tuple of the two MarbleEaterNodes\n",
    "    \"\"\"\n",
    "\n",
    "    eater_positions = [(-10, 0), (10, 0)]\n",
    "    node_positions = [(0, -5), (-5, 0), (5, 0), (0, 5)]\n",
    "    input_region_pos = np.array((-2.5, -1))\n",
    "    input_region_size = np.array((5, 2))\n",
    "    mass = 1\n",
    "    radius = 0.5\n",
    "\n",
    "    attraction_function = NewtonianGravity()\n",
    "\n",
    "    nodes = gen_nodes(attraction_function, mass, node_positions)\n",
    "    eater_nodes = gen_eater_nodes(attraction_function, mass, \n",
    "                                  radius, eater_positions)\n",
    "    model = NenwinModel(nodes+eater_nodes)\n",
    "    input_placer = VelInputPlacer(input_region_pos, input_region_size)\n",
    "\n",
    "    return model, input_placer, eater_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL, INPUT_PLACER, EATERS = gen_architecture()"
   ]
  },
  {
   "source": [
    "## Training setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.93587, -5.1008 ,  4.5367 ,  1.3866 ]])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "LOSS_POS_WEIGHT = 1\n",
    "LOSS_VEL_WEIGHT = 1\n",
    "DATASET = load_banknote_dataset()\n",
    "train_set = tuple(DATASET.iter_train())\n",
    "train_set[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainer(model: NenwinModel,\n",
    "                   input_placer: InputPlacer,\n",
    "                   output_nodes: Iterable[MarbleEaterNode],\n",
    "                   dataset: BanknoteDataset,\n",
    "                   loss_pos_weight: float,\n",
    "                   loss_vel_weight: float) -> NenwinTrainer:\n",
    "\n",
    "    loss_funct = NenwinLossFunction(output_nodes, model, loss_vel_weight,\n",
    "                                    loss_pos_weight)\n",
    "    optim = torch.optim.Adam(model.parameters())\n",
    "    filename_gen = FilenameGenerator(BANKNOTE_CHECKPOINT_DIR, \"BANKNOTE_\", \".txt\")\n",
    "    trainer = NenwinTrainer(model, loss_funct, optim, filename_gen,\n",
    "                            input_placer, dataset)\n",
    "    return trainer\n",
    "\n",
    "TRAINER = create_trainer(MODEL, \n",
    "                         INPUT_PLACER, \n",
    "                         EATERS,\n",
    "                         DATASET,\n",
    "                         loss_pos_weight= LOSS_POS_WEIGHT,\n",
    "                         loss_vel_weight = LOSS_VEL_WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 4.1605 11.2196 -3.6136 -4.0819]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7eae672bd6ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0mnum_steps_till_read_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mdo_validate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                     checkpoint_interval)\n\u001b[0m",
      "\u001b[0;32m~/Vault/Documents/bachelor_3/honors/nenwin/nenwin/backprop/trainer.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(self, num_epochs, step_size, num_steps_till_read_output, do_validate, checkpoint_interval)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             epoch_loss = self.__run_one_trainset_epoch(\n\u001b[0;32m---> 89\u001b[0;31m                 step_size, num_steps_till_read_output)\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_train_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Vault/Documents/bachelor_3/honors/nenwin/nenwin/backprop/trainer.py\u001b[0m in \u001b[0;36m__run_one_trainset_epoch\u001b[0;34m(self, step_size, num_steps_till_read_output)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mmarbles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__input_placer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarblelize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_marbles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarbles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Vault/Documents/bachelor_3/honors/nenwin/nenwin/grid_input_placer.py\u001b[0m in \u001b[0;36mmarblelize_data\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_pos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgrid_cell_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             positions = [offset + grid_cell_size[dim]*cell\n\u001b[0;32m---> 82\u001b[0;31m                          for cell in range(input_data.shape[dim])]\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mpositions_per_dim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "step_size = 0.1\n",
    "num_steps_till_read_output = int(1 / step_size)\n",
    "do_validate = True\n",
    "checkpoint_interval = 100\n",
    "\n",
    "TRAINER.run_training(num_epochs,\n",
    "                    step_size,\n",
    "                    num_steps_till_read_output,\n",
    "                    do_validate,\n",
    "                    checkpoint_interval)"
   ]
  }
 ]
}