The classification scheme as described above has been used on the Banknote Authentication dataset \cite{banknote_paper} (downloaded from \cite{banknote_download}\footnote{Note that this page explains the labels the wrong way around. See \cite{genuine_class_banknote}}).
This particular dataset was chosen because it required only few particles to encode the input and the output:
\begin{enumerate}
	\item There are only two distinct labels (binary classification).
	\item Each input vector contains only four elements.
\end{enumerate}

The implementation is available at \url{https://github.com/Nifrec/nenwin} under the open-source AGPL-3.0 licence\cite{AGPL_3}.

Note that these experiments are intended to give an indication of trainability and parameter sensitivity of the \textsc{Nenwin} framework. 
A full search-space evaluation is beyond the scope of this report (and would require a more optimized implementation).

\subsection{Dataset description}
The classification task is to classify banknotes as genuine or as forgeries, 
given four derived features from the images. 
Class 0 has 762 samples and represent the genuine banknotes,
class 1 has 610 represents the forgeries. 
This split is close to a 50\%/50\% split, meaning that random classification would yield an accuracy around 50\%.
The derived features (of Wavelet-transformed image) have been pre-computed, and are:
\begin{enumerate}
	\item Variance
	\item Skewness
	\item Kurtosis
	\item Entropy
\end{enumerate}

For the experiments, the samples of the dataset were shuffled in a random order and split in a 80\%/10\%/10\% train/validation/test set split. This split was cached and reused for each experiment (i.e. each experiment had the same train, validation and test set).

\subsection{Architectures}
Three different architectures have been designed and were evaluated. See also Fig. \ref{fig:banknote_architectures}.

\paragraph{Architecture A:}
\begin{itemize}
	\item Vertices of input region: (-2.5, -1), (-2.5, 1), (2.5, -1), (2.5, 1).
	\item 4 non-output nodes surrounding the input region (positions (0, -5), (-5, 0), (5, 0) and (0, 5)).
	\item MarbleEaterNodes (output Nodes) at (-10, 0) and (10, 0).
\end{itemize}

\paragraph{Architecture B:}
Same as Architecture A, but with additional Nodes at (2.5, 2.5), (-2.5, 2.5), (2.5, -2.5), (-2.5, -2.5).

\paragraph{Architecture C:}
\begin{itemize}
	\item Vertices of input region: (0, 0), (0, 1), (6, 0), (6, 1).
	\item 5 non-output nodes surrounding the input region (positions (1, 2), (2, 3), (3, 2), (4, 3) and (5, 2)).
	\item MarbleEaterNodes (output Nodes) at (1, 4) and (5, 4).
\end{itemize}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/architecture_A.pdf}
		\label{fig:architecture_a}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/architecture_B.pdf}
		\label{fig:architecture_b}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/architecture_C.pdf}
		\label{fig:architecture_c}
	\end{subfigure}

	\caption{\textsc{Nenwin} Architectures used for training on the banknote dataset.}
	\label{fig:banknote_architectures}
\end{figure}

\subsection{Setup}
For each experiment, one of the architectures A, B or C was generated, and other hyperparameters were chosen.
In particular, the batch size and the input placer were varied. 

The training procedure used was as follows:
during each epoch, the samples of the training set are iterated in a random order. 
Each sample in the training set converted to Marbles by the chosen input placer.
Then the simulation is run until at least one MarbleEaterNode ate a Marble, 
or until a maximum number of steps is reached. 
Then the loss is computed according to \eqref{eq:classification_loss_full},
and gradients of parameters with respect to this loss are computed via backpropagation.
Finally the parameters are updated using the Adam optimization technique.

The average gradient is used in case the batch size is larger than 1. 
The samples are still processed in sequential order, but the gradients of their losses are accumulated.
The Adam update rule is only used after the last sample in the batch.

Because of practical resource and time constraints, this maximum amount of timesteps was set to only 20 steps, with a stepsize of 0.1. 
With this configuration it took several hours to train an architecture\footnote{Training was run on a a Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz.}. 

\subsection{Results}

Numerical results of various training-runs have been summarized in Table \ref{table:banknote_exp}. 

\begin{table}[hb]
	\begin{tabular}{lll|lll}
		\textbf{Architecture} &
		\textbf{Timesteps} &
		\textbf{\begin{tabular}[c]{@{}l@{}}input\\ placer\end{tabular}} &
		\textbf{\begin{tabular}[c]{@{}l@{}}batch\\ size\end{tabular}} &
		\textbf{\begin{tabular}[c]{@{}l@{}}validation\\ accuracy\end{tabular}} &
		\textbf{\begin{tabular}[c]{@{}l@{}}training\\ loss\end{tabular}} \\ \hline
		A & 20 & VelInputPlacer  & 1 & 0.17518 & 20307.7 \\
		A & 20 & VelInputPlacer  & 2 & 0.13869 & 21173.4 \\
		A & 20 & VelInputPlacer  & 5 & 0.06569 & 28580.8 \\
		A & 20 & MassInputPlacer & 1 & 0.0     & 13.5    \\
		B & 20 & VelInputPlacer  & 1 & 0.12409 & 24788.9 \\
		C & 20 & VelInputPlacer  & 1 & 0.08029 & 33143.6
	\end{tabular}
	\caption{Results of training a \textsc{Nenwin} architecture on the banknote-authentication dataset. Each row represents an independent training run. The accuracy is the total amount of correct predictions divided over the size of the validation set. Absence of output is counted as a wrong prediction. The loss accumulation of losses of the last epoch of the train set.}
	\label{table:banknote_exp}
\end{table}

Using the VelInputPlacer appears to providing poor results. A plot of the performance per epoch shows that the training initially makes progress, but converge too quickly to mediocre performance. See Fig. \ref{fig:velinputplacer_performance}. One possible explanation is that the VelInputPlacer gives the input Marbles a high initial velocity in varying directions. With a very limited amount of Nodes the model may not posses the flexibility handle all combinations of initial velocity directions of the Marbles. 

\begin{figure}
	\centering
	\includegraphics[scale=0.4]{figures/A_batch1_velinputplacer.pdf}
	\caption{Performance of Architecture A as measured during each epoch of training. The VelInptPlacer was used to map the four banknote features to four Marbles. The batch size was 1. Note how the improvement of the accuracy on the validation set already seems to destabilize and cease rapid growth in less than 20 epochs. Given that the decrease in train set loss also flattened out at this time, it would seem unlikely that running more epochs would significantly increase the validation set accuracy.}
	\label{fig:velinputplacer_performance}
\end{figure}

\clearpage